{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99593cc2",
   "metadata": {},
   "source": [
    "## FASE 2: PRÉ-PROCESSAMENTO DOS DADOS\n",
    "\n",
    "#### Esta fase justifica as transformações feitas para tornar o modelo de Machine Learning viável.\n",
    "\n",
    "### 2.1: Pré-processamento e Escalonamento de Dados\n",
    "\n",
    "## Fase 2: Preparação dos Dados (Data Preparation)\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "Após entender a estrutura e o comportamento dos dados na Fase 1 (EDA), esta fase tem como objetivo preparar o dataset para a modelagem de forma **consistente, reproduzível e livre de vazamento de dados**. Em vez de aplicar transformações “soltas” diretamente nos DataFrames, todo o pré-processamento é encapsulado em um objeto único (`preprocessor.joblib`), que será reutilizado na fase de modelagem (notebook 3) e, posteriormente, no deployment (notebook 4).\n",
    "\n",
    "Essa abordagem segue as boas práticas de projetos de Machine Learning em produção:\n",
    "\n",
    "- garante que as mesmas transformações sejam aplicadas em treino, validação, teste e produção;\n",
    "- reduz risco de data leakage (parâmetros de imputação e escala são aprendidos apenas no conjunto de treino);\n",
    "- facilita integração com pipelines de validação cruzada e tuning de hiperparâmetros.\n",
    "\n",
    "---\n",
    "\n",
    "### Etapas\n",
    "\n",
    "1. **Carregamento do dataset bruto**  \n",
    "   - Leitura do arquivo `creditcard.csv` diretamente do repositório do Kaggle.  \n",
    "   - Verificação das dimensões iniciais do dataset.\n",
    "\n",
    "2. **Definição formal da variável-alvo e das features**  \n",
    "   - Definição de `Class` como coluna alvo (`target_col`).  \n",
    "   - Identificação de todas as colunas numéricas.  \n",
    "   - Remoção explícita da coluna alvo da lista de features numéricas.  \n",
    "   - Criação de listas `numeric_cols` e `categorical_cols` que serão a referência oficial do projeto.\n",
    "\n",
    "3. **Construção do pré-processador oficial (`preprocessor`)**  \n",
    "   - Imputação de valores ausentes em variáveis numéricas com a mediana (`SimpleImputer(strategy=\"median\")`).  \n",
    "   - Escalonamento robusto (`RobustScaler`) para reduzir o impacto de outliers em features como `Time` e `Amount`.  \n",
    "   - (Opcional) Imputação e codificação one-hot para variáveis categóricas, caso existam em versões futuras do dataset.  \n",
    "   - Encapsulamento dessas transformações em um `ColumnTransformer`, compatível com `Pipeline`, `Cross-Validation` e `RandomizedSearchCV`.\n",
    "\n",
    "4. **Persistência do pré-processador**  \n",
    "   - Salvamento do objeto `preprocessor` em `artifacts/preprocessor.joblib`.  \n",
    "   - Salvamento de metadados de pré-processamento (nome da coluna alvo, listas de colunas numéricas e categóricas) em `artifacts/preprocessing_metadata.json`.  \n",
    "   - Esses artefatos serão carregados na Fase 3 (Modelagem) para construir os pipelines completos de treino e tuning, garantindo consistência entre notebooks e ambientes.\n",
    "\n",
    "---\n",
    "\n",
    "### Benefícios\n",
    "\n",
    "- **Consistência entre as fases do projeto**: o mesmo pré-processamento é utilizado em todas as etapas (treino, validação, teste, produção), evitando discrepâncias entre notebooks e scripts.\n",
    "- **Redução de vazamento de dados (data leakage)**: parâmetros de imputação e escala são aprendidos apenas nos dados de treino, quando o `preprocessor` é usado em conjunto com validação cruzada e pipelines completos.\n",
    "- **Modularidade e reuso**: o objeto `preprocessor` pode ser acoplado a diferentes modelos (Logistic Regression, Random Forest, XGBoost, LightGBM, CatBoost) sem duplicação de código.\n",
    "- **Facilidade de experimentação**: qualquer alteração nas regras de pré-processamento (imputação, escala, encoding) é feita em um único ponto (`pipeline_new.py`), refletindo automaticamente em todas as fases do projeto.\n",
    "- **Preparação para produção**: o uso de artefatos versionáveis (`joblib` + `metadata.json`) aproxima o notebook de um fluxo real de MLOps, facilitando o deploy e o monitoramento do modelo final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abaa41",
   "metadata": {},
   "source": [
    "### Célula 1 – Imports e ligação com pipeline_new.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc41fa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import joblib # Para salvar os objetos transformados\n",
    "\n",
    "project_root = r\"C:\\Users\\debor\\OneDrive\\Github\\FraudSense\"\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from pipeline_new import build_preprocessor, save_pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b12d4e",
   "metadata": {},
   "source": [
    "### Célula 2 – Carregamento dos dados com load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9905688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados: (284807, 31)\n"
     ]
    }
   ],
   "source": [
    "# Célula 2 Carregamento dos Dados\n",
    "def load_data():\n",
    "    \"\"\"Carrega o dataset original\"\"\"\n",
    "    # Se você já baixou no notebook 1, o arquivo deve estar em cache ou na pasta local\n",
    "    # Ajuste o caminho se necessário, ou use o kagglehub novamente\n",
    "    import kagglehub\n",
    "    path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
    "    file_path = os.path.join(path, 'creditcard.csv')\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "df = load_data()\n",
    "print(f\"Dados carregados: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e097097",
   "metadata": {},
   "source": [
    "### Célula 3 – Definição de colunas oficiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6141ca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas numéricas usadas no pipeline:\n",
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9'] ... (total: 30 )\n",
      "Colunas categóricas usadas no pipeline:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Célula 3: definição de colunas para o pipeline oficial\n",
    "\n",
    "# Target\n",
    "target_col = \"Class\"\n",
    "\n",
    "# Features numéricas (no dataset creditcard.csv, praticamente todas são numéricas)\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Remover a coluna alvo da lista de numéricas\n",
    "if target_col in numeric_cols:\n",
    "    numeric_cols.remove(target_col)\n",
    "\n",
    "# Se você tiver colunas categóricas em algum momento futuro, coloque aqui\n",
    "categorical_cols = []  # no creditcard.csv original não há categorias\n",
    "\n",
    "print(\"Colunas numéricas usadas no pipeline:\")\n",
    "print(numeric_cols[:10], \"... (total:\", len(numeric_cols), \")\")\n",
    "print(\"Colunas categóricas usadas no pipeline:\")\n",
    "print(categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28c130",
   "metadata": {},
   "source": [
    "### Célula 4 – Construção do pré-processador oficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3d7ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pré-processador criado com sucesso!\n",
      "<class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "Transformers definidos:\n",
      " • num: Pipeline -> 30 colunas\n"
     ]
    }
   ],
   "source": [
    "# Célula 4: construção do pré-processador oficial para o projeto\n",
    "\n",
    "from pipeline_new import build_preprocessor\n",
    "\n",
    "preprocessor = build_preprocessor(\n",
    "    numeric_cols=numeric_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_strategy=\"median\",  # igual ao que você já vinha usando\n",
    "    scale=\"robust\"              # coerente com seu uso de RobustScaler\n",
    ")\n",
    "\n",
    "print(\"Pré-processador criado com sucesso!\")\n",
    "print(type(preprocessor))\n",
    "print(\"Transformers definidos:\")\n",
    "for name, trans, cols in preprocessor.transformers:\n",
    "    print(f\" • {name}: {type(trans).__name__} -> {len(cols)} colunas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780358a4",
   "metadata": {},
   "source": [
    "### Célula 5 – Salvando o pré-processador oficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa3c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline salvo em artifacts\\preprocessor.joblib\n",
      "\n",
      "Pré-processador salvo com sucesso!\n",
      "Caminho: artifacts\\preprocessor.joblib\n",
      "Este é o objeto que será usado na fase de modelagem (notebook 3).\n"
     ]
    }
   ],
   "source": [
    "# Célula 5: salvar o pré-processador oficial\n",
    "\n",
    "from pipeline_new import save_pipeline\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "preprocessor_path = os.path.join(\"artifacts\", \"preprocessor.joblib\")\n",
    "save_pipeline(preprocessor, preprocessor_path)\n",
    "\n",
    "print(\"\\nPré-processador salvo com sucesso!\")\n",
    "print(\"Caminho:\", preprocessor_path)\n",
    "print(\"Este é o objeto que será usado na fase de modelagem (notebook 3).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2679da",
   "metadata": {},
   "source": [
    "## ⚠️IMPORTANTE:\n",
    "\n",
    "### As funções abaixo (stratified_split_data, robust_scaling, balance_classes, etc.)  foram utilizadas no desenvolvimento e entendimento do pré-processamento,  mas o pipeline final de produção está centralizado no módulo `pipeline_new.py`  e no objeto `preprocessor` salvo em artifacts/preprocessor.joblib.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84e908",
   "metadata": {},
   "source": [
    "### Análise Inicial para Definir Estratégia de Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714cb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8: Análise Inicial para Definir Estratégia de Pré-processamento (SEM DATA LEAKAGE)\n",
    "def pre_processing_analysis(df):\n",
    "    \"\"\"Analisa os dados para definir a melhor estratégia de pré-processamento\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\" ANÁLISE PARA DEFINIÇÃO DE PRÉ-PROCESSAMENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Análise das features numéricas\n",
    "    print(\"\\n **ANÁLISE DAS FEATURES NUMÉRICAS:**\")\n",
    "    \n",
    "    # Separar features PCA das originais\n",
    "    pca_features = [f'V{i}' for i in range(1, 29)]\n",
    "    original_features = ['Time', 'Amount']\n",
    "    \n",
    "    print(f\"• Features PCA (V1-V28): {len(pca_features)}\")\n",
    "    print(f\"• Features originais: {original_features}\")\n",
    "    \n",
    "    # Estatísticas das features originais\n",
    "    print(f\"\\n **ESTATÍSTICAS DAS FEATURES ORIGINAIS:**\")\n",
    "    stats_originais = df[original_features].describe()\n",
    "    print(stats_originais)\n",
    "    \n",
    "    # Verificar distribuição das features PCA\n",
    "    print(f\"\\n **DISTRIBUIÇÃO DAS FEATURES PCA:**\")\n",
    "    pca_stats = df[pca_features].describe().T[['mean', 'std', 'min', 'max']]\n",
    "    print(pca_stats.head(8))  # Mostrar apenas as primeiras\n",
    "    \n",
    "    # **REMOVIDA a análise de correlação com target aqui**\n",
    "    # Isso será feito APENAS no conjunto de treino depois da divisão\n",
    "    \n",
    "    print(f\"\\n **IMPORTANTE:** Análise de correlação será realizada\")\n",
    "    print(\"  apenas no conjunto de TREINO para evitar data leakage\")\n",
    "    \n",
    "    return pca_features, original_features\n",
    "\n",
    "# Executar análise\n",
    "pca_features, original_features = pre_processing_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e1b56",
   "metadata": {},
   "source": [
    "### Divisão Estratificada dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 9: Divisão Estratificada dos Dados\n",
    "def stratified_split_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Divide os dados de forma estratificada mantendo a proporção de classes\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\" DIVISÃO ESTRATIFICADA DOS DADOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Separar features e target\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    print(f\"• Shape original: {X.shape}\")\n",
    "    print(f\"• Proporção da classe 1 (fraude): {y.mean():.4%}\")\n",
    "    \n",
    "    # Divisão estratificada\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        stratify=y, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Divisão do temporário em treino e validação\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size=0.25,  # 0.25 * 0.8 = 0.2 do total\n",
    "        stratify=y_temp, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n **DIVISÃO CONCLUÍDA:**\")\n",
    "    print(f\"• Conjunto de Treino: {X_train.shape} ({len(X_train)/len(X):.1%})\")\n",
    "    print(f\"• Conjunto de Validação: {X_val.shape} ({len(X_val)/len(X):.1%})\")\n",
    "    print(f\"• Conjunto de Teste: {X_test.shape} ({len(X_test)/len(X):.1%})\")\n",
    "    \n",
    "    print(f\"\\n **DISTRIBUIÇÃO DAS CLASSES NOS CONJUNTOS:**\")\n",
    "    for nome, conjunto in [('Treino', y_train), ('Validação', y_val), ('Teste', y_test)]:\n",
    "        prop_fraude = conjunto.mean()\n",
    "        print(f\"• {nome}: {conjunto.shape[0]:,} amostras ({prop_fraude:.4%} fraudes)\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Executar divisão\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfbf75",
   "metadata": {},
   "source": [
    "### Escalonamento Robustos das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 10: Escalonamento Robustos das Features\n",
    "def robust_scaling(X_train, X_val, X_test, original_features):\n",
    "    \"\"\"\n",
    "    Aplica escalonamento robusto às features originais 'Time' e 'Amount'\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\" ESCALONAMENTO ROBUSTO DAS FEATURES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Criar cópias para não modificar os originais\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_val_scaled = X_val.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    # Inicializar scalers robustos (menos sensível a outliers)\n",
    "    scaler_time = RobustScaler()\n",
    "    scaler_amount = RobustScaler()\n",
    "    \n",
    "    print(\" **APLICANDO ESCALONAMENTO:**\")\n",
    "    \n",
    "    # Escalonar feature 'Time'\n",
    "    X_train_scaled['Time'] = scaler_time.fit_transform(X_train[['Time']])\n",
    "    X_val_scaled['Time'] = scaler_time.transform(X_val[['Time']])\n",
    "    X_test_scaled['Time'] = scaler_time.transform(X_test[['Time']])\n",
    "    print(\"• Feature 'Time' escalonada com RobustScaler\")\n",
    "    \n",
    "    # Escalonar feature 'Amount'\n",
    "    X_train_scaled['Amount'] = scaler_amount.fit_transform(X_train[['Amount']])\n",
    "    X_val_scaled['Amount'] = scaler_amount.transform(X_val[['Amount']])\n",
    "    X_test_scaled['Amount'] = scaler_amount.transform(X_test[['Amount']])\n",
    "    print(\"• Feature 'Amount' escalonada com RobustScaler\")\n",
    "    \n",
    "    # Verificar estatísticas após escalonamento\n",
    "    print(f\"\\n **ESTATÍSTICAS APÓS ESCALONAMENTO (TREINO):**\")\n",
    "    stats_apos = X_train_scaled[original_features].describe()\n",
    "    print(stats_apos)\n",
    "    \n",
    "    print(f\"\\n **JUSTIFICATIVA TÉCNICA:**\")\n",
    "    print(\"• RobustScaler: Remove mediana e escala com IQR, robusto a outliers\")\n",
    "    print(\"• Preserva distribuição original melhor que StandardScaler\")\n",
    "    print(\"• Ideal para dados financeiros com outliers\")\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler_time, scaler_amount\n",
    "\n",
    "# Aplicar escalonamento\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, scaler_time, scaler_amount = robust_scaling(\n",
    "    X_train, X_val, X_test, original_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21370b",
   "metadata": {},
   "source": [
    "### Análise de Correlação APENAS no Treino (SEM DATA LEAKAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84cccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 10.5: Análise de Correlação APENAS no Treino (SEM DATA LEAKAGE)\n",
    "def analyze_training_correlations(X_train_scaled, y_train):\n",
    "    \"\"\"Analisa correlações APENAS no conjunto de treino\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\" ANÁLISE DE CORRELAÇÃO NO CONJUNTO DE TREINO\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\" **EVITANDO DATA LEAKAGE**\")\n",
    "    \n",
    "    # Combinar features e target do treino\n",
    "    train_data = X_train_scaled.copy()\n",
    "    train_data['Class'] = y_train.values\n",
    "    \n",
    "    # Calcular correlações\n",
    "    correlations = train_data.corr()['Class'].sort_values(ascending=False)\n",
    "    \n",
    "    # Top 10 features mais correlacionadas\n",
    "    print(\"\\n **TOP 10 FEATURES MAIS CORRELACIONADAS COM FRAUDE:**\")\n",
    "    print(\"(Ordem decrescente de importância)\")\n",
    "    \n",
    "    top_positive = correlations[1:11]  # Excluir a correlação perfeita com ela mesma\n",
    "    top_negative = correlations[-10:]\n",
    "    \n",
    "    print(\"\\nCorrelações POSITIVAS (mais associadas a fraude):\")\n",
    "    for feature, corr in top_positive.head(5).items():\n",
    "        print(f\"  • {feature}: {corr:.4f}\")\n",
    "    \n",
    "    print(\"\\nCorrelações NEGATIVAS (menos associadas a fraude):\")\n",
    "    for feature, corr in top_negative.tail(5).items():\n",
    "        print(f\"  • {feature}: {corr:.4f}\")\n",
    "    \n",
    "    # Visualização\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_corr = pd.concat([top_positive.head(5), top_negative.tail(5)])\n",
    "    colors = ['coral' if x > 0 else 'skyblue' for x in top_corr]\n",
    "    \n",
    "    ax = top_corr.sort_values().plot(kind='barh', color=colors)\n",
    "    plt.title('Top 10 Features Mais Correlacionadas com Fraude\\n(Conjunto de Treino)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Coeficiente de Correlação')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Insights\n",
    "    print(f\"\\n **INSIGHTS:**\")\n",
    "    print(f\"• Feature mais associada à fraude: {correlations.index[1]} ({correlations[1]:.4f})\")\n",
    "    print(f\"• Feature menos associada à fraude: {correlations.index[-1]} ({correlations[-1]:.4f})\")\n",
    "    print(f\"• Total de features com correlação > 0.1: {(correlations.abs() > 0.1).sum()}\")\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Executar análise APÓS o escalonamento\n",
    "correlations = analyze_training_correlations(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d2ec4",
   "metadata": {},
   "source": [
    "### Técnicas de Balanceamento de Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d862e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 11: Técnicas de Balanceamento de Classes\n",
    "def balance_classes(X_train_scaled, y_train, strategy='smote'):\n",
    "    \"\"\"\n",
    "    Aplica técnicas de balanceamento para lidar com o desbalanceamento\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\" TÉCNICAS DE BALANCEAMENTO DE CLASSES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\" **DISTRIBUIÇÃO ANTES DO BALANCEAMENTO:**\")\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    for classe, count in zip(unique, counts):\n",
    "        tipo = \"FRAUDE\" if classe == 1 else \"NORMAL\"\n",
    "        print(f\"• Classe {classe} ({tipo}): {count:,} amostras\")\n",
    "    \n",
    "    # Diferentes estratégias de balanceamento\n",
    "    if strategy == 'smote':\n",
    "        print(f\"\\n **APLICANDO SMOTE (Synthetic Minority Over-sampling Technique):**\")\n",
    "        smote = SMOTE(\n",
    "            random_state=42,\n",
    "            sampling_strategy='auto',  # Balanceia para 50%-50%\n",
    "            k_neighbors=5\n",
    "        )\n",
    "        X_balanced, y_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "        technique = \"SMOTE\"\n",
    "        \n",
    "    elif strategy == 'undersample':\n",
    "        print(f\"\\n **APLICANDO UNDERSAMPLING (Subamostragem da Maioria):**\")\n",
    "        undersampler = RandomUnderSampler(\n",
    "            random_state=42,\n",
    "            sampling_strategy='auto'\n",
    "        )\n",
    "        X_balanced, y_balanced = undersampler.fit_resample(X_train_scaled, y_train)\n",
    "        technique = \"Undersampling\"\n",
    "        \n",
    "    elif strategy == 'combine':\n",
    "        print(f\"\\n **APLICANDO COMBINAÇÃO (SMOTE + Undersampling):**\")\n",
    "        # Primeiro oversampling com SMOTE\n",
    "        smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "        X_temp, y_temp = smote.fit_resample(X_train_scaled, y_train)\n",
    "        \n",
    "        # Depois undersampling\n",
    "        undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "        X_balanced, y_balanced = undersampler.fit_resample(X_temp, y_temp)\n",
    "        technique = \"SMOTE + Undersampling\"\n",
    "    \n",
    "    else:\n",
    "        # Sem balanceamento\n",
    "        print(f\"\\n  **SEM BALANCEAMENTO (apenas para comparação):**\")\n",
    "        X_balanced, y_balanced = X_train_scaled.copy(), y_train.copy()\n",
    "        technique = \"Sem balanceamento\"\n",
    "    \n",
    "    print(f\"\\n **DISTRIBUIÇÃO APÓS {technique.upper()}:**\")\n",
    "    unique_after, counts_after = np.unique(y_balanced, return_counts=True)\n",
    "    for classe, count in zip(unique_after, counts_after):\n",
    "        tipo = \"FRAUDE\" if classe == 1 else \"NORMAL\"\n",
    "        print(f\"• Classe {classe} ({tipo}): {count:,} amostras\")\n",
    "    \n",
    "    print(f\"\\n **ESTATÍSTICAS DO BALANCEAMENTO:**\")\n",
    "    print(f\"• Amostras antes: {len(X_train_scaled):,}\")\n",
    "    print(f\"• Amostras depois: {len(X_balanced):,}\")\n",
    "    print(f\"• Proporção fraudes antes: {y_train.mean():.4%}\")\n",
    "    print(f\"• Proporção fraudes depois: {y_balanced.mean():.4%}\")\n",
    "    \n",
    "    return X_balanced, y_balanced, technique\n",
    "\n",
    "# Aplicar SMOTE (estratégia recomendada)\n",
    "X_balanced, y_balanced, technique_used = balance_classes(X_train_scaled, y_train, strategy='smote')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d50a38",
   "metadata": {},
   "source": [
    "### Seleção de Features Baseada em Importância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 12: Seleção de Features Baseada em Importância\n",
    "def feature_selection(X_balanced, y_balanced, X_val_scaled, X_test_scaled, k_features=20):\n",
    "    \"\"\"\n",
    "    Seleciona as k features mais importantes usando teste ANOVA F\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\" SELEÇÃO DE FEATURES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\" **FEATURES DISPONÍVEIS:** {X_balanced.shape[1]}\")\n",
    "    \n",
    "    # Seleção de features usando ANOVA F-value\n",
    "    selector = SelectKBest(score_func=f_classif, k=k_features)\n",
    "    \n",
    "    print(f\"\\n **SELECIONANDO AS {k_features} MELHORES FEATURES...**\")\n",
    "    X_selected = selector.fit_transform(X_balanced, y_balanced)\n",
    "    X_val_selected = selector.transform(X_val_scaled)\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "    \n",
    "    # Obter scores e nomes das features selecionadas\n",
    "    feature_scores = selector.scores_\n",
    "    feature_names = X_balanced.columns\n",
    "    selected_mask = selector.get_support()\n",
    "    \n",
    "    # Criar DataFrame com scores\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'F_Score': feature_scores,\n",
    "        'Selecionada': selected_mask\n",
    "    }).sort_values('F_Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n **TOP {k_features} FEATURES SELECIONADAS:**\")\n",
    "    print(feature_importance[feature_importance['Selecionada'] == True][['Feature', 'F_Score']].head(k_features))\n",
    "    \n",
    "    print(f\"\\n **RESUMO DA SELEÇÃO:**\")\n",
    "    print(f\"• Features originais: {X_balanced.shape[1]}\")\n",
    "    print(f\"• Features selecionadas: {X_selected.shape[1]}\")\n",
    "    print(f\"• Features removidas: {X_balanced.shape[1] - X_selected.shape[1]}\")\n",
    "    \n",
    "    # Plotar importância das features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    features_selecionadas = feature_importance[feature_importance['Selecionada'] == True].head(15)\n",
    "    features_nao_selecionadas = feature_importance[feature_importance['Selecionada'] == False].head(10)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(features_selecionadas['Feature'], features_selecionadas['F_Score'], color='skyblue')\n",
    "    plt.title('Top 15 Features Selecionadas\\n(maior F-Score)', fontweight='bold')\n",
    "    plt.xlabel('F-Score (ANOVA)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(features_nao_selecionadas['Feature'], features_nao_selecionadas['F_Score'], color='lightcoral')\n",
    "    plt.title('Top 10 Features Não Selecionadas\\n(menor F-Score)', fontweight='bold')\n",
    "    plt.xlabel('F-Score (ANOVA)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return X_selected, X_val_selected, X_test_selected, selector, feature_importance\n",
    "\n",
    "# Aplicar seleção de features\n",
    "X_selected, X_val_selected, X_test_selected, selector, feature_importance = feature_selection(\n",
    "    X_balanced, y_balanced, X_val_scaled, X_test_scaled, k_features=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d64171",
   "metadata": {},
   "source": [
    "### Pipeline Completo de Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b838b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 13: Pipeline Completo de Pré-processamento\n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"\n",
    "    Cria um pipeline reprodutível para pré-processamento\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\" PIPELINE DE PRÉ-PROCESSAMENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Definir steps do pipeline\n",
    "    preprocessing_steps = {\n",
    "        '1. Divisão Estratificada': 'train_test_split com stratify',\n",
    "        '2. Escalonamento RobustScaler': 'Time e Amount',\n",
    "        '3. Balanceamento SMOTE': 'Synthetic Minority Over-sampling',\n",
    "        '4. Seleção de Features': 'SelectKBest com ANOVA F-test',\n",
    "        '5. Preservação Conjuntos': 'Validação e Teste sem vazamento'\n",
    "    }\n",
    "    \n",
    "    print(\" **STEPS DO PIPELINE:**\")\n",
    "    for step, descricao in preprocessing_steps.items():\n",
    "        print(f\"  • {step}: {descricao}\")\n",
    "    \n",
    "    # Criar dicionário com objetos para reproduzibilidade\n",
    "    preprocessing_objects = {\n",
    "        'scalers': {\n",
    "            'time_scaler': scaler_time,\n",
    "            'amount_scaler': scaler_amount\n",
    "        },\n",
    "        'selector': selector,\n",
    "        'feature_importance': feature_importance,\n",
    "        'technique_used': technique_used\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n **OBJETOS PARA REPRODUTIBILIDADE:**\")\n",
    "    for obj_name, obj in preprocessing_objects.items():\n",
    "        if obj_name != 'feature_importance':  # Não printar o DataFrame completo\n",
    "            print(f\"  • {obj_name}: {type(obj).__name__}\")\n",
    "    \n",
    "    return preprocessing_objects\n",
    "\n",
    "# Criar pipeline\n",
    "preprocessing_objects = create_preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060b5da",
   "metadata": {},
   "source": [
    "### Resumo Final do Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 14: Resumo Final do Pré-processamento\n",
    "def preprocessing_summary(X_selected, X_val_selected, X_test_selected, y_balanced, y_val, y_test):\n",
    "    \"\"\"\n",
    "    Fornece um resumo completo do pré-processamento\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\" RESUMO FINAL - PRÉ-PROCESSAMENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n **CONFIGURAÇÃO FINAL DOS CONJUNTOS:**\")\n",
    "    \n",
    "    conjuntos = {\n",
    "        'Treino (Balanceado)': (X_selected, y_balanced),\n",
    "        'Validação': (X_val_selected, y_val),\n",
    "        'Teste': (X_test_selected, y_test)\n",
    "    }\n",
    "    \n",
    "    for nome, (X, y) in conjuntos.items():\n",
    "        print(f\"\\n **{nome}:**\")\n",
    "        print(f\"  • Shape: {X.shape}\")\n",
    "        print(f\"  • Amostras: {X.shape[0]:,}\")\n",
    "        print(f\"  • Features: {X.shape[1]}\")\n",
    "        print(f\"  • Proporção Fraudes: {y.mean():.4%}\")\n",
    "        print(f\"  • Número de Fraudes: {y.sum():,}\")\n",
    "    \n",
    "    print(f\"\\n **PRINCIPAIS TRANSFORMAÇÕES APLICADAS:**\")\n",
    "    transformacoes = [\n",
    "        \"Divisão estratificada (80-20)\",\n",
    "        \"Escalonamento RobustScaler em 'Time' e 'Amount'\", \n",
    "        \"Balanceamento com SMOTE\",\n",
    "        \"Seleção das 20 melhores features (ANOVA F-test)\",\n",
    "        \"Preservação de validação/teste sem vazamento\"\n",
    "    ]\n",
    "    \n",
    "    for i, transf in enumerate(transformacoes, 1):\n",
    "        print(f\"  {i}. {transf}\")\n",
    "    \n",
    "    print(f\"\\n **ESTATÍSTICAS GLOBAIS:**\")\n",
    "    total_amostras = len(X_selected) + len(X_val_selected) + len(X_test_selected)\n",
    "    print(f\"• Total de amostras processadas: {total_amostras:,}\")\n",
    "    print(f\"• Features reduzidas de {df.shape[1]-1} para {X_selected.shape[1]}\")\n",
    "    print(f\"• Taxa de redução dimensional: {((df.shape[1]-1 - X_selected.shape[1])/(df.shape[1]-1)*100):.1f}%\")\n",
    "    \n",
    "    print(f\"\\n **PRÓXIMOS PASSOS - MODELAGEM:**\")\n",
    "    proximos_passos = [\n",
    "        \"1. Treinar modelos na versão balanceada (X_selected, y_balanced)\",\n",
    "        \"2. Validar performance na versão original (X_val_selected, y_val)\", \n",
    "        \"3. Testar final no conjunto blind (X_test_selected, y_test)\",\n",
    "        \"4. Focar em Recall para detecção de fraudes\",\n",
    "        \"5. Usar Stratified K-Fold para validação robusta\"\n",
    "    ]\n",
    "    \n",
    "    for passo in proximos_passos:\n",
    "        print(f\"  {passo}\")\n",
    "\n",
    "# Executar resumo\n",
    "preprocessing_summary(X_selected, X_val_selected, X_test_selected, y_balanced, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1d443",
   "metadata": {},
   "source": [
    "### Salvamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c702a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 15: Salvamento dos Dados Processados\n",
    "def save_processed_data(X_selected, X_val_selected, X_test_selected, y_balanced, y_val, y_test):\n",
    "    \"\"\"\n",
    "    Salva os dados processados para uso na fase de modelagem\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\" SALVANDO DADOS PROCESSADOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Criar dicionário com todos os conjuntos\n",
    "    processed_data = {\n",
    "        'X_train': X_selected,\n",
    "        'X_val': X_val_selected, \n",
    "        'X_test': X_test_selected,\n",
    "        'y_train': y_balanced,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'feature_names': X_selected.columns if hasattr(X_selected, 'columns') else [f'V{i}' for i in range(X_selected.shape[1])]\n",
    "    }\n",
    "    \n",
    "    # Salvar usando pickle\n",
    "    import pickle\n",
    "    \n",
    "    with open('processed_fraud_data.pkl', 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "    \n",
    "    print(\" **DADOS SALVOS COM SUCESSO:**\")\n",
    "    print(\"• Arquivo: 'processed_fraud_data.pkl'\")\n",
    "    print(\"• Contém: X_train, X_val, X_test, y_train, y_val, y_test, feature_names\")\n",
    "    print(\"• Pronto para a fase de modelagem!\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Salvar dados\n",
    "processed_data = save_processed_data(X_selected, X_val_selected, X_test_selected, y_balanced, y_val, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
